{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LMLSTM(nn.Module):\n",
    "    \"\"\"Simple LSMT-based language model\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        num_steps,\n",
    "        batch_size,\n",
    "        vocab_size,\n",
    "        num_layers,\n",
    "        dp_keep_prob,\n",
    "        ):\n",
    "        super(LM_LSTM, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dp_keep_prob = dp_keep_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(1 - dp_keep_prob)\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=embedding_dim,\n",
    "                            num_layers=num_layers, dropout=1\n",
    "                            - dp_keep_prob)\n",
    "        self.sm_fc = nn.Linear(in_features=embedding_dim,\n",
    "                               out_features=vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range = 0.1\n",
    "        self.word_embeddings.weight.data.uniform_(-init_range,\n",
    "                init_range)\n",
    "        self.sm_fc.bias.data.fill_(0.0)\n",
    "        self.sm_fc.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.num_layers, self.batch_size,\n",
    "                self.embedding_dim).zero_()),\n",
    "                Variable(weight.new(self.num_layers, self.batch_size,\n",
    "                self.embedding_dim).zero_()))\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        embeds = self.dropout(self.word_embeddings(inputs))\n",
    "        (lstm_out, hidden) = self.lstm(embeds, hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        logits = self.sm_fc(lstm_out.view(-1, self.embedding_dim))\n",
    "        return (logits.view(self.num_steps, self.batch_size,\n",
    "                self.vocab_size), hidden)\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load glove embeddings\n",
    "# 2. Map texts to indexes based on glove embeddings, calculate OOV %, freeze layer\n",
    "# 3. Train language model on unlabelled corpus\n",
    "# 4. \n",
    "#\n",
    "#\n",
    "#\n",
    "def batches(file):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def run_epoch(\n",
    "    model,\n",
    "    data,\n",
    "    is_train=False,\n",
    "    lr=1.0,\n",
    "):\n",
    "    \"\"\"Runs the model on the given data.\"\"\"\n",
    "\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    epoch_size = (len(data) // model.batch_size - 1) // model.num_steps\n",
    "    start_time = time.time()\n",
    "    hidden = model.init_hidden()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    for (step, (x, y)) in enumerate(batches()):\n",
    "        inputs = Variable(torch.from_numpy(x.astype(np.int64)).transpose(0,\n",
    "                     1).contiguous()).cuda()\n",
    "        model.zero_grad()\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        (outputs, hidden) = model(inputs, hidden)\n",
    "        targets = Variable(torch.from_numpy(y.astype(np.int64)).transpose(0,\n",
    "                     1).contiguous()).cuda()\n",
    "        tt = torch.squeeze(targets.view(-1, model.batch_size\n",
    "                           * model.num_steps))\n",
    "\n",
    "        loss = criterion(outputs.view(-1, model.vocab_size), tt)\n",
    "        costs += loss.data[0] * model.num_steps\n",
    "        iters += model.num_steps\n",
    "\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 0.25)\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(-lr, p.grad.data)\n",
    "            if step % (epoch_size // 10) == 10:\n",
    "                print('{} perplexity: {:8.2f} speed: {} wps'.format(step\n",
    "                        * 1.0 / epoch_size, np.exp(costs / iters),\n",
    "                        iters * model.batch_size / (time.time()\n",
    "                        - start_time)))\n",
    "    return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "print 'Vocabluary size: {}'.format(vocab_size)\n",
    "model = LMLSTM(\n",
    "    embedding_dim=args.hidden_size,\n",
    "    num_steps=args.num_steps,\n",
    "    batch_size=args.batch_size,\n",
    "    vocab_size=vocab_size,\n",
    "    num_layers=args.num_layers,\n",
    "    dp_keep_prob=args.dp_keep_prob,\n",
    "    )\n",
    "model.cuda()\n",
    "lr = args.inital_lr\n",
    "\n",
    "# decay factor for learning rate\n",
    "\n",
    "lr_decay_base = 1 / 1.15\n",
    "\n",
    "# we will not touch lr for the first m_flat_lr epochs\n",
    "\n",
    "m_flat_lr = 14.0\n",
    "\n",
    "print '########## Training ##########################'\n",
    "for epoch in range(args.num_epochs):\n",
    "    lr_decay = lr_decay_base ** max(epoch - m_flat_lr, 0)\n",
    "    lr = lr * lr_decay  # decay lr if it is time\n",
    "    train_p = run_epoch(model, train_data, True, lr)\n",
    "    print 'Train perplexity at epoch {}: {:8.2f}'.format(epoch, train_p)\n",
    "    print 'Validation perplexity at epoch {}: {:8.2f}'.format(epoch,\n",
    "            run_epoch(model, valid_data))\n",
    "print '########## Testing ##########################'\n",
    "model.batch_size = 1  # to make sure we process all the data\n",
    "print 'Test Perplexity: {:8.2f}'.format(run_epoch(model, test_data))\n",
    "with open(args.save, 'wb') as f:\n",
    "    torch.save(model, f)\n",
    "print '########## Done! ##########################'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
